---
permalink: /slides/lecture12.html
---
<!DOCTYPE html>
<html>
  <head>
    <title>Lecture 12: Plan Recognition</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; font-size: 2em; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      p { font-size: 1.25em; }
      div { font-size: 1.25em; }
      li { font-size: 1.25em; }
      li p { line-height: 1.25em; font-size: 1.25em; }
      .red { color: #fa0000; }
      .large { font-size: 2em; }
      
      .small li {  font-size: 1em; }
      
      .ssmall li {  font-size: 0.9em; }
      
      .oneline li { font-size: 1.25em; line-height: 1.25em; }
      .oneline li p { font-size: 1.25em; line-height: 1.25em; }
      .oneline p { font-size: 1.25em; line-height: 1.25em; }
      .oneline div { font-size: 1.25em; line-height: 1.25em; }
      
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      
      .medium li {  font-size: 1.1em; }
     
      .mmedium li {  font-size: 1.05em; }
      
        
      .left-column {
        color: #777;
        width: 40%;
        height: 92%;
        float: left;
      }
        .left-column h2:last-of-type, .left-column h3:last-child {
          color: #000;
        }
      .right-column {
        width: 60%;
        float: right;
        padding-top: 1em;
        font-size: 0.8em;
      }
      
       .right-column1 {
        width: 60%;
        float: right;
        padding-top: 1em;
        font-size: 0.75em;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Artificial Intelligence: Planning

### Plan Recognition

---

<img src="/CI-2807/assets/img/posterII2019.png" width="100%"/>

---

# Review: The Planning Problem

A planning problem consists of three parts:

  * A definition of the current state of the world
  
  * A definition of a desired state of the world
  
  * A definition of the actions the agent can take

---

# Observation

* Imagine we want to cooperate with an agent (or a human)

* We can see which actions they are performing

* Now we need to figure out what they want to achieve

---

class: medium

# Plan Recognition 

* Goal Recognition: Given some observed actions, figure out which goal the observed agent is trying to achieve

* Plan Recognition: Given some observed actions, figure out which plan the observed agent is likely to follow 

* Activity Recognition: Given some observed actions, figure out which of them are relevant to an activity performed by the observed agent 

Confusing: All of them were at some point or another called "Plan Recognition"

---

# Intention Recognition 

* More confusion: Some people also use "intention recognition"

* Unfortunately, this has some non-research connotations of "mind reading"

* Philosophers also have their own (different) ideas of what "intentions" are 

* We will be clear, and us "Plan Recognition" for the task of determining the *plan* that the observed agent is (likely) following

---

# Applications 

* Games (cooperative and competitive)

* Assistive care for the elderly

* Software help 

* Security

* Cooking 

* Etc.

---

# Plan Recognition Approaches 

* Plan library: Match observed actions against known plans 

* Statistical methods: Collect data from many users to predict likely follower actions

* Planning: What plan would *you* execute if you had done the observed actions 

---

# Plan Recognition: Example 

<img src="/PF-3335/assets/img/planrecognition.png" width="80%"/>

---

class: center, middle 

# Plan Recognition as Planning 

---

# Assumptions 

* We know the initial state of the world

* We observe complete actions 

* We observe them in order 

* We know what goals the observed agent could potentially be trying to achieve

* We believe the observed agent follows the optimal plan 

---

# The Plan Recognition Problem 

Given:

  * A set of observed actions 
  
  * A set of potential goals 
  
Find:

  * A set of plans the agent is likely to follow 
  
---

# A different perspective 

* If we know the initial state and the potential goals, we can just find a plan for each of them 

* Then we compare the plans we found with the actions the agent performed 

* If there's a match, we found a potential solution

* What could possibly go wrong?

---

# Optimality

* We assume the agent follows "the" optimal plan 

* What if there is more than one option?

* The agent may find one optimal plan 

* We find a different one that is also optimal 

* Those two don't match ...

---

class: medium

# Incorporating Observed Actions

* What we want is that our plans *have to* include the observed actions 

* We can include this in the goals!

* We create a new predicate `observed`, which has one parameter: an action/step number

* For each observed action `i` we create a **new** action that has an additional precondition `(not (observed i))` and an additional effect `(observed i)`

* We add **all** `(observed i)` atoms to each of the goals

---

# Plan Recognition as Planning 

* To perform the plan recognition, we now have to run the planner **twice** for each goal 

* One time, we try to find a plan given the initial state, with the unmodified actions and goal 

* The second time, we try to find a plan with the modified actions and goal 

* If (and only if) these two plans have the same length, do we consider the resulting plan a possibility

---

# Performance

* As mentioned, we need to use an optimal planner *twice* for each possible goal 

* This will be computationally expensive

* How about we use a heuristic planner that may not be optimal?

* We may overlook a few possible plans, or find a few extra ones
  
---

# Evaluation

* Let's say we have produced a set of plans that we believe the agent is following

* How do we determine if we "did well"?

* What was our goal?

* What can we expect?

---

# Evaluation: Precision and Recall 

* Precision: What percentage of produced plans are actually reasonable?

* Recall: What percentage of reasonable plans do we produce?

Both of these measures are important! 

We can use them to find out how good our "non-optimal" planner did

---

# Plan Recognition: Ideal Solution?

* Precision and Recall tell us how well an algorithm does 

* But what do we actually want?

* Also, does the *domain* have any effect on how good we can do?

* What if it's impossible to predict what the observed agent is trying to do?

---

# Worst Case Distinctiveness

<img src="/PF-3335/assets/img/wcd.png" width="60%"/>

---

# Worst Case Distinctiveness

* Worse Case Distinctiveness is a measure of a *domain* 

* It basically says how long we have to wait *in the worst case* before we can figure out what the observed agent is up to

* This can be arbitrarily "bad" (imagine an "infinite" corridor with two doors at the end ...)

* In practice we may want to modify our domain to improve the situation

---

# Worst Case Distinctiveness: Domain Modification

<img src="/PF-3335/assets/img/wcdfix.png" width="100%"/>

---

class: center, middle 

# Extensions

---

# Extensions

* Partial plans

* Observations 

* Noise

* Continual Plan Recognition

* Suboptimality

---

# Partial Plans 

<img src="/PF-3335/assets/img/planrecognition_partial.png" width="80%"/>

---

# Observations 

* So far we have assumed we can observe actions

* What if we can just observe the world?

* We will have to deduce which actions the agent performed in order to enact the changes we observe

* There may be multiple options for some changes

---

# Noise 

* Maybe we can't observe the *full* action, but only parts of it 

* Or we can't observe the *full* world 

* We may have to use some sort of fuzzy reasoning (or a possible worlds model!) to represent the world 

---

# Continual Plan Recognition 

* In many domains, like games, we will perform plan recognition over and over (e.g. every turn)

* But many of the computations will be repeated 

* If we design a planner that keeps track of expanded states (and discards the ones corresponding to actions that did not happen), we can make these computations more efficient

---

# Homework 

* No more homework 

* Next week we will wrap up the class with some applications and an outlook

* There will also be a discussion of the planning competition
  
---

class: small

# References

  * [Plan Activity and Intent Recognition Tutorial](http://www.planrec.org/Tutorial/Resources_files/pair-tutorial.pdf)
  
  * [Resources for Plan, Activity, Behavior, and Intent Recognition Research](http://www.planrec.org/Resources.html)
  
  * [Plan Recognition as Planning](https://sites.google.com/site/prasplanning/home)
  
  * [Goal Recognition Design](https://www.aaai.org/ocs/index.php/ICAPS/ICAPS14/paper/viewFile/7814/8023)

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
    <script type='text/javascript' src='https://kitchwww.github.io/midi/midi.js'></script>
    <script>
      function p(){
        MIDIjs.initAll();
        MIDIjs.play('/PF-3341/assets/img/sugerencia.midi');
        }
    
      var slideshow = remark.create();
      
      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });

      MathJax.Hub.Configured();
    </script>
  </body>
</html>
