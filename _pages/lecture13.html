
<!DOCTYPE html>
<html>
  <head>
    <title>Lecture 13: Conclusion</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; font-size: 2em; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      p { font-size: 1.25em; }
      div { font-size: 1.25em; }
      li { font-size: 1.25em; }
      li p { line-height: 1.25em; font-size: 1.25em; }
      .red { color: #fa0000; }
      .large { font-size: 2em; }
      
      .small li {  font-size: 1em; }
      
      .ssmall li {  font-size: 0.9em; }
      
      .oneline li { font-size: 1.25em; line-height: 1.25em; }
      .oneline li p { font-size: 1.25em; line-height: 1.25em; }
      .oneline p { font-size: 1.25em; line-height: 1.25em; }
      .oneline div { font-size: 1.25em; line-height: 1.25em; }
      
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      
      .medium li {  font-size: 1.1em; }
     
      .mmedium li {  font-size: 1.05em; }
      
        
      .left-column {
        color: #777;
        width: 40%;
        height: 92%;
        float: left;
      }
        .left-column h2:last-of-type, .left-column h3:last-child {
          color: #000;
        }
      .right-column {
        width: 60%;
        float: right;
        padding-top: 1em;
        font-size: 0.8em;
      }
      
       .right-column1 {
        width: 60%;
        float: right;
        padding-top: 1em;
        font-size: 0.75em;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Artificial Intelligence: Planning

### Conclusion

---

class: center, middle

# Current Developments

---

# Current Developments

- Planning is still a highly active field of research

- The International Conference on Automated Planning and Scheduling (ICAPS) is the premiere venue for planning research

- They also hold various competitions every year

- For example, the First Unsolvability International Planning Competition in 2016

---

# Recent Research

- Novel Heuristics

- Novel Applications

- Integration with Machine Learning

- Extensions to the formalism, like temporal and probabilistic planning

---

# Some Applications

- Logistics (RoboCup Logistics League)

- Ride sharing

- Video Games

- Narrative Generation

---

class: center, middle

# Goal-Oriented Action Planning

---

# Goal-Oriented Action Planning



---

class: center, middle

# Narrative Generation as Planning

---

# Narratives

* A narrative is often considered to consist of two parts:

  - Story: What is happening in the world (the "plot")

  - Discourse: What the audience is told (the "telling")

* We can use a planner to generate each of these two parts

---

# Story Generation

- We have an initial state of the world

- We have actions the different characters can perform

- The author defines a goal for the story

- For example: In a detective story the goal is that there is a murder and that the detective identifies the murderer

---

# Story Example

* Sherlock Holmes goes to Irene Adler's house

* Sherlock Holmes stabs Irene Adler

* Sherlock Holmes identifies himself as the killer

* The End.

---

# Story Generation as Planning

- Planners are usually good at finding efficient solutions

- Stories are not efficient!

- However, each *character* should act rationally, i.e. efficiently

- We could use one planner per character ...

---

# Intentional Planning

- With one planner per character we may never find a path to the story goal

- Instead, we use a "trick": We tell the planner what each character wants to achieve

- The planner may then only use actions for a character that helps them achieve their goal

- But the overall plan still has to solve the story goal!

- Implementations: IPOCL, CPOCL

---

class: center, middle

# Compilation

---

# Feature Creep

- New applications have led to the need for new features in planners

- For example, intentional planning for stories, or constraints on the plan

- But classical planning is already very expressive!

- We can "compile away" certain things

---

# Compiling Intentions

- In Intentional Planning the planner may only add actions that help a character achieve their goal

- But that's just another precondition!

- For each predicate, such as `has`, we add an additional predicate `intends-has`

- Then we add a new action for each such intention that has the additional precondition of that intention

---

# Why Compilation?

- IPOCL can solve intentional planning problems!

- To plan a variation of the Aladdin story it needs 12h

- The compiled version can be run using *any* state of the art planner

- Even with significantly more predicates due to the compilation, a solution was found in under a minute!

- Using compilation we can make use of all recent advances in classical planning!

---

class: center, middle

# Planner Selection

---

# Planner Selection

- No heuristic works well for all problems

- In fact, no planner works well for all problems

- What if we have "every" possible planner available?

- We would want to pick the best planner for each problem!

---

# Planner Selection

- We could run all planners in parallel

- As soon as one finishes, we return its solution and terminate all others

- Problem: We "need" 1 CPU per planner (or equivalent computation resources)

---

# Problem Structure

- Let's run each planner on a couple of benchmark problems and time it

- We then figure out which planner is the fastest for each of these problems

- Ideally, when we look at the problems each planner is good at, we can identify some common "structure"

- Then we use each planner on problems that have the structure of problems it is good at

---

class: medium

# Machine Learning?

- Identifying a common structure may be tricky to do manually

- Machine Learning is good at finding patterns!

- We could train a neural network on a representation of each problem and the time needed by a planner

- Then we can use the neural network to predict how long a particular planner may need on a new problem!

- Of course, this requires a proper encoding, and comes with all the other challenges of using Machine Learning

---

class: center, middle

# Machine Learning

---

# Machine Learning and Planning

- There are many ways Machine Learning and Planning can be used together

- Generally, planning is good when the domain is known, but the solution structure may not be

- On the other hand, Machine Learning is good when the solution structure is known, but he domain may not be

- We can use each one to complement the other

---

# Reinforcement Learning

A Reinforcement Learning Problem consists of:

- A state of the world

- Actions, the agent can take

- A reward function for the agent

--

Sounds a lot like planning?

---

# Reinforcement Learning

- In Reinforcement Learning, we tell the agent how many "points" it gets for reaching a final state

- The idea is that the agent "explores" the environment using some sequence of actions until it reaches a final state

- Then it uses the points it got to assign a value to each action it took to reach that state

- By repeating this "often", the agent learns the value of "each" action in "each" state

---

# Learning a Domain

- From observing how an agent interacts with a world, we may be able to learn domain actions

- For example, if we observe the world before and after some actions we can construct a model of what these actions may have done

- We may then use a planner to find a solution for some new problem

---

# Guiding the Agent

- In order to learn the agent needs to explore the world

- This exploration follows some exploration policy

- We may use a plan as this exploration policy (with some random noise, perhaps)

- Instead of exploring the environment completely randomly (initially), the agent uses this plan as a guide

---

class: small

# Wumpus World

- The Wumpus World domain is actually one of the standard textbook examples for Reinforcement Learning!

- The agent visits the world thousands of times and walks about randomly to learn which moves are good and which are bad

- Then it can visit a new world and knows not to run into the Wumpus (for example)

- Instead of moving about randomly, we can use a planner to provide a "good" path

- But maybe the world is too big? Our planner can operate on a simplified version, because it just needs to produce a guideline



---

class: center, middle

# Conclusion

---

# Conclusion

- Planning is very central to my research interests

- I really enjoyed teaching this class

- I hope I did not scare y'all away from this fascinating topic

- If you want to know more about my research, and/or do a project with me, let me know

---

class: medium

# Next Semester

- Please participate in the class evaluation!

- It helps me improve the course and my teaching in general

- It also helps the adminstration to decide which courses to offer

- There is also the Machine Learning and Statistical Learning class next semester, if you liked my teaching style and/or are interested in the topic!

---

class: center, middle

# Planning Competition

---

class: medium

# Planning Competition

- For the planning competition, I selected several "hard" problems

- My planner could still solve all of them in under 10 minutes

- I ran each planner on each of these problems

- The fastest planner for each problem got 5 points, the second fastest 3, the third fastest 1

- If a planner crashed, timed out or produced an invalid solution, it got -1 points

---

# Planning Competition: Third Place


---

# Planning Competition: Second Place

---

# Planning Competition: First Place

---

# Planning Competition: All Results

---

class: small

# References

  * [ICAPS 2019 Proceedings](https://aaai.org/ojs/index.php/ICAPS)
  
  * [Narrative Planning: Compilations to Classical Planning](https://arxiv.org/abs/1401.5856)

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
    <script>
    
      var slideshow = remark.create();
      
      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });

      MathJax.Hub.Configured();
    </script>
  </body>
</html>